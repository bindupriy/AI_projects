{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "201701040_Lab6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqnTD3bWUTaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "import operator\n",
        "import os\n",
        "import xml.etree.ElementTree as etree\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xqt4grVbU6",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwyAwO_hUTat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textprocessing(txt):\n",
        "  txt = txt.translate(str.maketrans('','',string.punctuation))\n",
        "  txt = re.sub(' \\d+',' ',txt)                               \n",
        "  txt = txt.lower()                                         \n",
        "  txt = '<s ' + txt + ' /s>'\n",
        "  txt = re.sub('\\s{2,}',' ',txt)\n",
        "  return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZIstCZ_UXn1",
        "colab_type": "text"
      },
      "source": [
        "**Extracting the sentences from the directories**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1fiNs4QUTa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/home/harsha/Downloads/dataset'\n",
        "test_path = '/home/harsha/Downloads/2007'\n",
        "def getlist(path):\n",
        "    indexes = []\n",
        "    stories = []\n",
        "\n",
        "    for files in os.listdir(path):\n",
        "        if(len(files) == 26):\n",
        "            indexes.append(files)\n",
        "        else:\n",
        "            stories.append(files)\n",
        "    return indexes,stories\n",
        "\n",
        "\n",
        "test_index,test_stories = getlist(test_path)\n",
        "train_index,train_stories = getlist(train_path)\n",
        "print(\"no of train stories\")\n",
        "len(train_stories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQXU57TFUTbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getSentences(path,indexes,stories):\n",
        "    sentences = []\n",
        "    n = int(len(indexes)/2)\n",
        "\n",
        "    for i in range(n):\n",
        "        filepath = os.path.join(path,indexes[i])\n",
        "        with open(filepath,'r') as f:\n",
        "                soup = BeautifulSoup(f, 'xml')\n",
        "                for text in soup.find_all('TEXT'):\n",
        "                    for line in text.text.splitlines()[7:-5]:\n",
        "                        if line : \n",
        "                            sentences.append(line)\n",
        "    n=200\n",
        "    for i in range(n):\n",
        "        filepath = os.path.join(path, stories[i])\n",
        "        with open(filepath,'r') as f:\n",
        "            soup = BeautifulSoup(f, 'xml')\n",
        "            for text in soup.find_all('TEXT'):\n",
        "                result = list(filter(lambda x: x, text.text.splitlines()[7:]))\n",
        "                for line in result:\n",
        "                    sentences.append(line)\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXiq8UFVUTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences = getSentences(train_path,train_index,train_stories)\n",
        "test_sentences = getSentences(test_path,test_index,test_stories)\n",
        "\n",
        "train = list(map(textprocessing,train_sentences))\n",
        "test = list(map(textprocessing,test_sentences))\n",
        "\n",
        "train_data = ' '.join(train)\n",
        "test_data = ' '.join(test)\n",
        "\n",
        "train_words = list(set(train_data.split()))\n",
        "test_words = list(set(test_data.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI6eAzPgaleC",
        "colab_type": "text"
      },
      "source": [
        "**Unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THoF0QC7UTbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_unigram = dict()\n",
        "test_unigram = dict()\n",
        "\n",
        "train_len = len(train_words)\n",
        "for word in train_words:\n",
        "    train_unigram[word] = len(re.findall(' ' + word + ' ', train_data))/train_len\n",
        "\n",
        "test_len = len(test_words)\n",
        "for word in test_words:\n",
        "    test_unigram[word] = len(re.findall(' ' + word + ' ', test_data))/test_len\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAtr-XCjfDE2",
        "colab_type": "text"
      },
      "source": [
        "**Bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "733vQRe_f3rL",
        "colab_type": "code",
        "colab": {},
        "outputId": "c915271a-bdb3-4ba3-ed79-3a41cc4154db"
      },
      "source": [
        "words = train_data.split()\n",
        "bigram = dict()\n",
        "start = time.process_time()         \n",
        "for i in range(len(words)-1):\n",
        "    temp = ' ' + words[i] + ' ' + words[i+1] + ' '\n",
        "    key = words[i] + ' ' + words[i+1]\n",
        "    try:\n",
        "        bigram[key] = len(re.findall(temp,train_data))/(train_unigram[words[i]]*train_len)\n",
        "    except:\n",
        "        continue\n",
        "print(time.process_time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "274.52249041000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS8hfrk8eGx8",
        "colab_type": "code",
        "colab": {},
        "outputId": "38717087-03f8-46f0-896b-35e70c00a9d7"
      },
      "source": [
        "def getBigramSent(current_word):\n",
        "    sent = current_word\n",
        "    current_word = current_word.split()\n",
        "    while(1):\n",
        "        search = current_word[-1] + ' '\n",
        "        temp = [ (key,w) for key,w in bigram.items() if key.startswith(search)]\n",
        "        temp.sort(key=operator.itemgetter(1))\n",
        "        temp.reverse()\n",
        "        print(temp[0])\n",
        "        current_word = temp[0][0].split()\n",
        "        sent = sent + \" \" + current_word[-1]\n",
        "        if('/s>' in current_word):\n",
        "            return sent\n",
        "\n",
        "current_word = '<s an'\n",
        "print(getBigramSent(current_word))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('an official', 0.026717557251908396)\n",
            "('official said', 0.3389830508474576)\n",
            "('said today', 0.23192019950124687)\n",
            "('today /s>', 0.08928571428571429)\n",
            "<s an official said today /s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqXf0GO6UTcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BigramTest(sent):\n",
        "  per_bi = 1\n",
        "  temp = sent.split()\n",
        "  N = len(temp)\n",
        "  for i in range(N-1):\n",
        "    bigr = temp[i] + ' ' + temp[i+1]\n",
        "    if(bi in bigram.keys() and bigram[bigr]!=0):\n",
        "        per_bi = per_bi*bigram[bigr]\n",
        "    else:\n",
        "        per_bi = per_bi*test_unigram[temp[i]]*test_unigram[temp[i+1]]\n",
        "        \n",
        "    \n",
        "    return per_bi**(-1/N)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnpMsTBDfH86",
        "colab_type": "text"
      },
      "source": [
        "**Trigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhmcKLV4UTb7",
        "colab_type": "code",
        "colab": {},
        "outputId": "083d37eb-d077-42bc-ac12-3ef683d9c248"
      },
      "source": [
        "trigram = dict()\n",
        "start = time.process_time()\n",
        "\n",
        "for i in range(len(words)-2):\n",
        "    temp = ' ' + words[i] + ' ' + words[i+1] + ' ' + words[i+2]  + ' '\n",
        "    key = words[i] + ' ' + words[i+1] + ' ' + words[i+2]\n",
        "    bigr = words[i] + ' ' + words[i+1]\n",
        "    try:\n",
        "        if bigram[bigr]:\n",
        "            trigram[key] = len(re.findall(temp,train_data))/(bigram[bigr]*train_len)\n",
        "    except:\n",
        "        continue\n",
        "print(time.process_time() - start)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240.59085678499997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwTH8pawUTcI",
        "colab_type": "code",
        "colab": {},
        "outputId": "2dd8e9c6-15a8-4675-95dd-2ee04b051ccc"
      },
      "source": [
        "def getTriSent(current_word):\n",
        "    sent = current_word\n",
        "    current_word = current_word.split()\n",
        "    while(1):\n",
        "        search = current_word[-1] + ' '\n",
        "        temp = [ (key,w) for key,w in trigram.items() if key.startswith(current_word[-1])]\n",
        "        temp.sort(key=operator.itemgetter(1))\n",
        "        temp.reverse()\n",
        "        print(temp[0])\n",
        "        current_word = temp[0][0].split()\n",
        "        sent = sent + \" \" + ' '.join(temp[0][0].split()[1:])\n",
        "        if('/s>' in current_word):\n",
        "            return sent\n",
        "current_word = '<s two buses'\n",
        "print('\\n ' + getTriSent(current_word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('buses police and', 0.00048344210780759005)\n",
            "('and destroyed cars', 0.14438804286520024)\n",
            "('cars the body', 0.0013697526387881718)\n",
            "('body part /s>', 0.0015309000080573685)\n",
            "\n",
            " <s two buses police and destroyed cars the body part /s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POXlulIeUTcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TrigramTest(sent):\n",
        "  per_tri = 1\n",
        "  temp = sent.split()\n",
        "  N = len(temp)\n",
        "  for i in range(N-2):\n",
        "    tri = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
        "    try:\n",
        "        if trigram[tri]:\n",
        "            per_tri = per_tri*trigram[tri]\n",
        "        else:\n",
        "            try:  \n",
        "                per_tri = prer_tri*test_unigram[temp[i]]*test_unigram[temp[i+1]]*test_unigram[temp[i+2]]\n",
        "            except:\n",
        "                continue\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "  return per_tri**(-1/N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlRXxH67UTcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_bigram = 1\n",
        "p_trigram = 1\n",
        "for lines in test:\n",
        "    p_bigram = p_bigram*BigramTest(lines)\n",
        "    p_trigram = p_trigram*TrigramTest(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uzhtefPZ90Y",
        "colab_type": "text"
      },
      "source": [
        "**Perplexities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxq8FvcBUTci",
        "colab_type": "code",
        "colab": {},
        "outputId": "ce325548-2adf-40da-ffb6-e97d5fc93442"
      },
      "source": [
        "print(\"perplexity for bigram\")\n",
        "p_bigram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perplexity for bigram\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.790398554129296e+286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ppJJXBUTcn",
        "colab_type": "code",
        "colab": {},
        "outputId": "5481b48d-ed94-480b-8e19-e0083e61b86c"
      },
      "source": [
        "print(\"preplexity for trigram\")\n",
        "p_trigram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preplexity for trigram\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59VtSfOOUjNc",
        "colab_type": "text"
      },
      "source": [
        "**Using inbuilt nltk**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30sGJMKfUTcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textprocessing2(txt):\n",
        "  txt = txt.translate(str.maketrans('','',string.punctuation))\n",
        "  txt = txt.lower()                                           #converts to lower case\n",
        "  txt = re.sub(' \\d+',' ',txt)                                #deleting the numbers\n",
        "  txt = re.sub('\\s{2,}',' ',txt)\n",
        "  return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR7UaCeYUTc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = list(map(textprocessing2,sentences_train))\n",
        "train_temp =[]\n",
        "for line in train:\n",
        "    train_temp.append(line.split())\n",
        "\n",
        "test = list(map(textprocessing2,sentences_test))\n",
        "test_temp =[]\n",
        "for line in test:\n",
        "    test_temp.append(line.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G20dpnIBUTc5",
        "colab_type": "code",
        "colab": {},
        "outputId": "12122bfb-9734-4ea7-d34b-ca37fe8cb8c1"
      },
      "source": [
        "!pip install -U pip\n",
        "!pip install -U dill\n",
        "!pip install -U nltk==3.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in ./ENTER/lib/python3.7/site-packages (20.0.2)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 584 kB/s eta 0:00:01\n",
            "\u001b[?25hBuilding wheels for collected packages: dill\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=77454 sha256=658921fea65235e53c166ddea0ca34679aeced8daa19f4d3cd4c4205d0b85318\n",
            "  Stored in directory: /home/harsha/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "Successfully built dill\n",
            "Installing collected packages: dill\n",
            "Successfully installed dill-0.3.1.1\n",
            "Collecting nltk==3.4\n",
            "  Downloading nltk-3.4.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 869 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in ./ENTER/lib/python3.7/site-packages (from nltk==3.4) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: singledispatch in ./ENTER/lib/python3.7/site-packages (from nltk==3.4) (3.4.0.3)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1435855 sha256=b2a035a43b395c3f3e0203a879f9c383f1f99ad7086eaa48a17e983249faf306\n",
            "  Stored in directory: /home/harsha/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.3\n",
            "    Uninstalling nltk-3.3:\n",
            "      Successfully uninstalled nltk-3.3\n",
            "Successfully installed nltk-3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI5xZxirUTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs2JcP0kUTdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "train, vocab = padded_everygram_pipeline(2, train_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtNjqzSSUTdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.lm import MLE\n",
        "lm1 = MLE(1)\n",
        "lm2 = MLE(2)\n",
        "lm3 = MLE(3)\n",
        "lm1.fit(train,vocab)\n",
        "lm2.fit(train,vocab)\n",
        "lm3.fit(train,vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-zs7Zi7U3jm",
        "colab_type": "text"
      },
      "source": [
        "**Perplexities of uni,bi,trigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CKWaIE4UTdT",
        "colab_type": "code",
        "colab": {},
        "outputId": "16ef3389-c3a4-412e-cc1c-9dce4e45d398"
      },
      "source": [
        "print(\"preplexity for unigram\")\n",
        "lm1.perplexity(test_temp[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preplexity for unigram\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtEU9NkUTda",
        "colab_type": "code",
        "colab": {},
        "outputId": "820a21a0-4a58-4e60-9ebc-a3e7bca07e0e"
      },
      "source": [
        "print(\"preplexity for bigram\")\n",
        "lm1.perplexity(test_temp[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preplexity for bigram\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXfJH-pGUTdf",
        "colab_type": "code",
        "colab": {},
        "outputId": "186ef063-c5fd-4eaa-9f92-913f98b88102"
      },
      "source": [
        "print(\"preplexity for trigram\")\n",
        "lm1.perplexity(test_temp[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preplexity for trigram\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjwo5q5uU-pZ",
        "colab_type": "text"
      },
      "source": [
        "**Sentence generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWf2lcpkUTdp",
        "colab_type": "code",
        "colab": {},
        "outputId": "db173b8d-654d-4013-f898-0767e67c5f58"
      },
      "source": [
        "print(lm1.generate(4, text_seed=['islamabad'], random_seed=2))\n",
        "print(lm1.generate(10,random_seed=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yesterday', 'were', 'up', 'under']\n",
            "['ways', 'to', 'use', 'trade', 'union', 'were', 'up', 'under', 'way', 'today']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVow8M-UTdt",
        "colab_type": "code",
        "colab": {},
        "outputId": "a493ae0e-1f4a-4bfa-9a61-150e67eebccc"
      },
      "source": [
        "print(lm1.generate(4, text_seed=['visit'], random_seed=2))\n",
        "print(lm1.generate(10,random_seed=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['to', 'use', 'trade', 'union']\n",
            "['ways', 'to', 'use', 'trade', 'union', 'were', 'up', 'under', 'way', 'today']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}